This document is for logging the day-by-day progress of my master's thesis.

* Week 2
** <2019-01-07 mon>
   The first day. Had [[file:./msc.org::*Startup meeting with Heintz][a startup meeting.]]

** <2019-01-08 tis>
   Met with Tiger, asked him [[file:./msc.org::tiger-question-1][about the structure of the thesis plan]]. 
   Wrote a draft introduction and aim chapter.
** <2019-01-09 ons>
   Spent most of the time reading on clustering and identifying
   motion patterns. Some stuff from the
   computer vision field: cite:Zhang2006Aug, cite:Tran2014Jun, and
   some stuff I got from Tiger:
   [[file:../../shared/fusion2015-tiger.pdf]],
   [[file:../../shared/overlapping-mixtures-of-gps.pdf]].
** <2019-01-10 tor>
   Got access to the "office"! One week until TP should be done so I
   started writing on related work.

** <2019-01-11 fre>
   Wrote related work on arrival time prediction (a couple of LSTMs). Continued writing
   related work for trajectory learning using GPs. There are issues with
   understanding [[file:../../shared/modeling-motion-patterns/energy-consumption-profile-using-gps.pdf][this paper]]

** <2019-01-12 lör>
   Wrote about the inverse Gaussian process in related work.

* Week 3
** <2019-01-14 mån>
    Tomorrow is hand in to Tiger for feedback on TP. I've been reading
    and writing a lot on the topic modeling approach to clustering
    trajectories. HDP, LDA seem central and different improvements
    have been made on them, like Dual-HDP, DDual-HDP, LC-LDA.

** <2019-01-15 tis>
   Had a feedback meeting with Tiger. Got a lot of good feedback. He
   also told me to write a pitch of the applications use, and to
   clearly address all sub-problems of the
   implementation. Additionally I should make a time plan for the
   projects milestones, but can wait with the exact week-to-week
   details until after the meeting with Heintz on Friday.

** <2019-01-16 ons>
   Finished a draft on the model pitch and addressing the
   sub-problems of the application. Also wrote the related work
   section on event detection based on automatic kernel structure discovery.
   Tomorrow I will proof read and write down a rough time plan for
   the project. 

** <2019-01-17 tor>
   Meeting with Heintz tomorrow. Proof read the entire thing and fixed
   grammatical mistakes. Also made a document for the time plan with
   the final deadlines in. The rest of the plan will be completed
   after the meeting with Heintz.

** <2019-01-18 fri>
   Had the with Heintz. He thought the plan was excellent and well
   written! The plan is to implement the proposed system and then
   evaluate improvements. To do this the evaluation and problem formulation
   need to be precisely formulated.
   After thinking about this for a while, it is really hard to
   measure quality of clustering.
** <2019-01-19 lör>
   Put down some theory on GPs from the TDDE19 project and changed
   some notation on the inverse GP synchronisation based on suggestions
   from Tiger.

* Week 4
** <2019-01-21 mån>
   I should make a time plan today. The first thing to put in in till
   be what to do this week, where there are mainly two options. I
   could either write the background chapter, or I could start with
   deriving the model. Deriving the model is harder, and more critical
   to the project so I think it will have to be that. This means that
   I have to settle on a specific model.

   After thinking a lot and talking to Tiger, the model I will go for
   is one where every trajectory \(T_{i}\) is modeled as \(f_{i} : \tau_{i} \mapsto s\)
   accompanied with \(g_{i} : \s \mapsto \tau_{i}\) and \(h_{i} :
   \tau_{i} \mapsto t\). That is, every trajectory gets a
   synchronisation GP that maps to a space where they can be compared
   with every other GP.
  
** <2019-01-22 tis>
   I keep wrestling with notation. There is quite a lot of it
   floating around, and denoting a specific trajectory, and new
   observations in particular is difficult.

   After finishing a draft describing the synchronisation model I can
   at least start addressing the problems with learning it. In
   particular, the data augmentation will be described.

   I have to redo my derivation of the model. I talked to Tiger about
   how to derive the model from the problem of
   comparing two trajectories. I think the line of thought is roughly:
   We want to measure distance as the orthogonal projection onto trajectories.
   This can be done by mapping X = (state . tau . state)(x) and taking the
   E[X]. So I guess we then could compute the distance as E[X]-x, but
   this is not what we do (why?). Instead we compute the likelihood
   P(E[X] | tau, Model).

** <2019-01-23 ons>
   The goal for today is to finish up the new model
   description. Tomorrow I want to be done with both describing the
   model, and explain the learning process, including data
   augmentation and stop compression.

   It has been hard. I have managed to explain the problem fairly well
   up until assessing the probabilistic stuff.

** <2019-01-24 tor>
   Today I'll finish up the model description section. In addition,
   I'll write about the issues of learning it.

   I got access to What's App, allowing me to get in touch with Tiger
   without walking all the way to his office.
** <2019-01-25 fre>
   I took the day off, since I had no feedback from Tiger.
   
* Week 5
** <2019-01-28 mån>
   Today I started implementing things. I wrote a module for learning
   trajectory models composing of function models and a notebook using
   said module. Unfortunately, there is an [[https://github.com/SheffieldML/GPy/issues/723][issue]] with
   plotting. Calling it gives 
   =ValueError: shape mismatch: value array of shape (200,1,1) could
   not be broadcast to indexing result of shape (1,200)=.
   
   I talked to Tiger about feedback on my report progress, and he will
   read it tomorrow. So I will talk to him about it the day after tomorrow.

   Until then I guess I'll just write the data chapter in the
   report. I'll start with that tomorrow, and if I get some response
   on the plotting issue, I will see to that as well. 

** <2019-01-29 tis>
   I got a response on the issue from yesterday, so I could probably
   continue the implementation. However, it would be nice to get the
   boring stuff out of the way as soon as possible, so I'll write the
   data chapter toady anyway. I've also got over six weeks until
   half-time meeting. Didn't have much focus today.

** <2019-01-30 ons>
   I tried to get plotting to work, to no avail. I even went home, got
   the requirements.txt file used for TDDE19, but even that doesn't
   work. I tried to use the plotly backend, but it refuses to be
   imported into a notebook. As a final solution I went online to get
   the matplotlib documentation and GPy source code to implement
   plotting myself, but something is seriously fucked up with the
   internet, since every page gives [[https://support.mozilla.org/en-US/kb/what-does-your-connection-is-not-secure-mean][this error]].

   I also tried to get predictions out 

** <2019-01-31 tor>
   I spent the day at home, because the reasons described
   yesterday. The issue on GitHub got reopened, so hopefully that will
   result in a fix on its own while I continue to work. I made quite
   some progress today though! I implemented training, plotting and
   storing models in a Python module. The final (and most difficult)
   step is the prediction step, which I might just finish this evening.
   Tiger also responded with feedback today, so I will work on the
   report tomorrow.

** <2019-02-01 fre>
   I've read through Tigers comments and have managed to work through
   most of them. Some of the math in the final prediction elude me, so
   I'm heading to his office this afternoon to ask some questions. 
   
   A lot of stuff in theory and intro should be condensed into the
   background chapter, and I think this is the right time to do it. It
   should establish a baseline of what I can expect the reader to know
   when they read the methods chapter.

   I sent a weekly summary to Tiger.
   
   I had a great meeting with Tiger with feedback on the plots I sent
   with the weekly summary. A lot of improvements was found:
   - Add linear kernel to \(g\)
   - Add linear kernel to \(h\)
   - Fix likelihood variance \(g\)
   - Respect aspect ratio when down
   - Interpolate data augmentation

** <2019-02-03 sön>
   Today I started implementing the improvements from the meeting with
   Tiger. It went great and the model looks a lot better! Left to do
   is is the mapping of lat/lon to an Euclidian space and linear
   kernel on \(g\).
   
* Week 6
  <2019-02-04 mån>
  Added linear kernel to \(g\). Started writing background chapter
  with starting machine learning. It is really hard. Really stressed
  out about Klarna as well.

  <2019-02-05 tis>
  Interview day with Klarna, and what a train-wreck of a day it
  was. Got a few sentences down on the train, but nothing much.

  <2019-02-06 ons>
  Took a break from writing and finished up the implementation on
  arrival time prediction. It is total shite, and only one prediction
  has any meaningful probability mass. It seems like the learned
  trajectories also have the exact same pattern every time, which
  makes me highly suspicious on the implementation. Need to look for
  bugs. 

  <2019-02-07 tor>
  (Forgot to write) Wrote background on machine learning. 

  <2019-02-08 fre>
  (Forgot to write) Almost finished with background on machine
  learning. Need to talk about marginal likelihood. 

* Week 7
  <2019-02-11 mån>
  Caught a cold over the weekend so I took they day off.

  <2019-02-12 tis>
  Had a meeting with Tiger and put down the final formal description
  of the model. Also discussed various improvements to the model, such
  as projecting onto Euclidian space before training GPs, how to
  properly normalise the arrival time prediction mixture, and adding
  "support GPs" to get a proper variance.

  <2019-02-13 ons>
  Implemented equirectangular projection as pre-processing step. This
  should do, as long as the data is not too spread out. If it is, this
  approximation will be shite. I have also discovered a problem with
  how the trajectories are normalised. Currently, they are normalised
  individually, but this is a problem since. The same trajectory won't
  look the same for every model. I think all trajectories for each
  segment need to be normalised in the same way. I should ask Tiger.

  <2019-02-14 tor>
  I proof read and rewrote some parts in the background
  chapter. Additionally I wrote about marginal likelihood and finished
  the section on compound kernel search. I removed MAPE from model
  evaluation. I need to ask Tiger about residual distribution. Should they be Gaussian in
  the end? Is the MSAE relevant because of this? 
  
* Week 8
  <2019-02-18 mån>
  I started to incorporate the formal model description into the
  methods chapter. It is hard to write, as always. What remains is how
  to refer to the synch-model, and the regular model, and to connect
  all GPs to final arrival time prediction.

  <2019-02-19 tis>
  Had a meeting with Tiger and asked him about error metrics and about
  coordinate systems for tau and the data. Also told him about Klarna,
  and he was kinda negative. Oh bother.

  <2019-02-20 ons>
  Implemented normalising all trajectories in the same coordinate
  system, ratio-preserving scaling, and ARD to get around lat and lon
  nog being euclidian. I got stuck at how to calculate data log
  likelihood, so I sent Tiger an email. I also started to revise the
  methods chapter, to structure it better with new parts of deriving
  the formal model, training it, and querying it.

  <2019-02-21 tor>
  Wrote some more on the methods chapter and mucked about with some
  bugs with normalising the data. 

  <2019-02-22 fre>
  There was problems with computing velocity. It performed some
  trigonometry that assumes angle and radius in a Euclidian plane, but
  this does not apply for latitude and longitude.  Now it is estimated
  as the difference between consequtive points.
  
  I solved some bugs in the arrival time prediction. I assume that the
  data log likelihood is just the sum of the independent log
  likelihoods, but I've yet to receive a response from Tiger on this question.

  Something seem to be off with the predictions though. They do not
  correspond to the plots of the individual models.
* Week 9
  <2019-02-25 mån>
    Forgot to write this day. Fixed some bugs in prediction. The log
    likelihood was computer completely wrong, but after reading Bishops
    I set it straight.

  <2019-02-26 tis>
    Forgot to write this day. Ironed out some additional bugs and wrote
    benchmarking functionality. The performance was terrible.

  <2019-02-27 ons>
    Hopefully fixed the last couple of bugs and plotting! 
    OMW to  meeting with Tiger. So far I have made the final model work
    on a few trajectories on a specific segment, hopefully it is only
    down to hyper parameters now.

    A lot of stuff need to be done:
      - Plot cumulative \(P(\mathcal{M}_k | \tau\)
      - Figure out the variance of \(P(t | \tau\)

  <2019-02-28 tor>
    Log likelihood of all models are quite high, so I tried to lower it
    by lowering the variance of the state models. However, there is a
    problem where the cumulative log likelihood of some highly unlikely
    model becomes -40000, and when normalising this blows everything
    up. I'm not sure how to handle this.

  <2019-03-01 fre>
    I tried to normalise velocity as well, and it seems to be reasonable
    in the cumulative log likelihood plots. However, the log likelihood
    when making predictions seem to be infinity because of super small
    covariances. Something is definitely off.

    I also need to put all the plots into the report, so I should save
    nice plots that appear during debugging. 

* Week 10
  <2019-03-04 mån>
  Changed my implementation of computing log likelihood to one a found
  in GPy and stuff does not seem to be infinity any more. The plots
  look really good, especially after fixed likelihood noise in /(h/).

  It seems like the correct model is easily out-voted even with no fixed
  likelihood. This causes mean predictions to give worse in-sample
  performance than mode predictions.

  I had a meeting with Tiger. It is clear that the "solution" to the
  numerical instability was nonsense. I think I will have to go back
  to manually computing the log likelihood.


  <2019-03-05 tis>
  It is clear that something is off when computing the determinant of
  /(\sigma/). I /think/ that this could be because of the velocity
  in either x- or y-direction being incredibly small. The numerical
  instability for segment 9 happens around halfway through the segment,
  which is where /(dy/) should be very small.

  I will give this idea a go when I get home.

  <2019-03-06 ons>
  I met with Tiger and figured out a lot of stuff. Arrival time
  clusters should be modeled as Gaussians witåh mean prior from /(h/)
  and /(sigma/) from accumulated model uncertainty. 

  <2019-03-07 tor>
  Today I worked with the GP prior for arrival time mean. Currently
  the variance is static to simulate accumulated uncertainty, but
  there are some confusion. Primarily the fact
  that they are all the same size, which doesn't make sense. However,
  should they only be based on /(h/) they will be incredibly uneven
  depending on how non-linear /(h/) is. Some compromise is wanted,
  however there is no real likelihood and prior to use Bayes theorem
  so I am at a loss. I will write this in the weekly report tomorrow.

  <2019-03-08 fre>
  Forgot to write. GPy started giving /negative covariances/ when
  estimating /(h/). Sad times. Managed to plot model performance
  segment-wise, but the in-sample performance was way too big, so
  there is some bug.

* Week 11
  <2019-03-11 mån>
  Figured out the bug with he performance. Stuff is still not very
  good out of sample, so I'm working on inducing inputs to be able to
  run with more trajectories. Currently is is done with 30. 

  Fucking GPy does not seem to support saving models with inducing
  inputs... Maybe it is possible to do it manually.

  <2019-03-12 tis>
  I did not manage to find a way to save inducing input models. I am
  not certain what the next step is. I suppose I could throw more
  trajectories in there and let it run, but it would take sooooo long.
  
  Basically the entire day has been spent training and evaluating
  models. It is not quite done yet, but I am off to meet Tiger. It
  evaluated on 50 trajectories using 5, 10, 20, 30, 50 models.

  <2019-03-13 ons>
  I forgot to write this day. However, I had the mentioned meeting
  With Tiger yesterday. We concluded that it was time to make
  pseudo-clusters instead of the static /(\sigma_n/) that is currently used.


  <2019-03-14 tor>
  I forgot to write this day. I spent most of the day preparing the
  presentation for the half-time seminar.

  <2019-03-15 fre>
  I attended the half time seminar. It went well and Heintz offered me a
  position as an industrial PhD. 

* Week 12
  <2019-03-16 mån>
  I am having issues with the pseudo-clustering. If one spans the
  clusters spatially this does not create clusters with uniform
  variance in tau-space. On the other hand, if one clusters in
  tau-space the different GPs become completely disconnected from an
  a priori specified cluster width.

  I am hoping to meet with Tiger to talk to him about it. However, he
  seems to be very busy today. Here's hoping.

  <2019-03-17 tis>
  I think I finally got everything to work with the new model. The in
  sample error from testing on 10 trajectories has a largest mean of 1
  second, which really is acceptable. The results will drop in after I
  leave, so I'll have to find out tomorrow. 

  The model evaluation is still incredibly slow. I will look into
  optimisations tomorrow when I know that the models are fine. Some
  things to look into are
  - Perform the pre-process step once for each segment
  - Compute model weights directly, without cumulative probabilities

  Lastly, it should now be possible to compute model probabilities
  from the current log likelihood. 

  <2019-03-18 ons>
  The in sample results seems good! The out of sample run had a bug so
  no results was produced.

  I discovered that the infinite log likelihood happens when I make a
  single computation in numpy. Looping over the individual samples and
  taking the cumulative sum of those avoids this issue. Why this is I
  currently do not know.

  <2019-03-19 tor>
  I met with Tiger and talked about a lot of things. The performance
  is still poor (it was variance and not standard deviation in the
  performance plots) and the log likelihoods were computed
  wrongly. This has fortunately been addressed, but I am afraid that
  things are taking longer than I had hoped for. I wanted to start
  working on event detection the coming week but it seems unlikely.

  <2019-03-25 mån>
  This was quite an awful Monday. The plotting has been acting up
  completely while trying to implement sparse GPs. I have gotten a
  single example to work in the notebook, so tomorrow I need to
  implement it in the project to enable storing them in a database. 

  <2019-03-26 tis>
  Today I finished the plotting with SI units, and the implementation
  of inducing inputs. Good times! After training 60 models for 2.5
  hours it turns out that the model is in fact terrible, even in sample.
  Huge let down of course. Maybe I can compute the amount of inducing
  points to use based on some minimum difference in KL-divergence from
  the true model. 

  I have to investigate this tomorrow. It might be a good timing to
  read up on the way inducing inputs are learned and write about that
  in the backgrounds chapter.

  <2019-03-27 ons>
  The performance is still shit. I discovered a bug where the 
  velocity of the final point got duplicated and broke performance for
  small trajectories. But even after fixing it, the model is still terrible. 
  I am going to have a quick meeting with Tiger tomorrow and talk
  about progressing the thesis project anyway. 

  Fucking latex-mode in emacs broke as well so I cant find any
  errors. I am beyond infuriated. 

  
* Week 19
  This week the report should be done and send to the opponent.

* Week 21
  Sometime this week the final presentation will take place

